{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task 2 Build a predictive model using a train/test split. Use the December fail logs as the test data.",
      "provenance": [],
      "mount_file_id": "1fDiVvXtuRhOnphbremUMuydURA5983gA",
      "authorship_tag": "ABX9TyOj0ZIgs5eicVaKH/HVdUYc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohailkhan/Aj_Machine_learning_master/blob/main/Task_2_Build_a_predictive_model_using_a_train_test_split_Use_the_December_fail_logs_as_the_test_data_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "address='/content/drive/MyDrive/Afujitsu/Task Data_ Server Down Data.xlsx'\n",
        "df = pd.read_excel (r'/content/drive/MyDrive/Afujitsu/Task Data_ Server Down Data.xlsx', sheet_name='Fail Logs')\n",
        "df1=df[0:40].drop(columns='Asset ID')\n",
        "df1['Fail Month'] = df1['Fail Month'].replace(['July','August','September','October','November','December'],[7,8,9,10,11,12])\n",
        "\n",
        "\n",
        "# making classes in downtime\n",
        "groups = ['Low', 'Med', 'High', 'Very High']\n",
        "df1['Downtime (GROUPS)']=pd.qcut(df1['Downtime (Min)'], q=4,labels=groups)\n",
        "\n",
        "# convert date time to numeric\n",
        "from datetime import datetime\n",
        "a = datetime.now()\n",
        "d=[]\n",
        "# Converting a to string in the desired format (YYYYMMDD) using strftime\n",
        "# and then to int.\n",
        "a = datetime.now()\n",
        "\n",
        "for date in df7['Fail Date']:\n",
        "  d.append(int(a.strftime('%Y%m%d')))\n",
        "df7['date_numeric']=d\n",
        "\n",
        "# X and Y for selecting base model\n",
        "X=df7.drop(columns=['Downtime (Min)','Downtime (GROUPS)','Fail Date','date_numeric'])\n",
        "Y=df7['Downtime (GROUPS)']\n",
        "\n",
        "# selecting decemeber as test\n",
        "train=df7.iloc[0:32,:]\n",
        "test=df7.iloc[32:40,:]\n",
        "\n",
        "\n",
        "\n",
        "# Standaderdize the X and Y data\n",
        "\n",
        "standard=StandardScaler()\n",
        "\n",
        "standard_X_train_fit=standard.fit(X)\n",
        "X_T=standard_X_train_fit.transform(X)\n",
        "\n",
        "\n",
        "# split Train and test data\n",
        "\n",
        "X_train=train.drop(columns=['Downtime (GROUPS)','Fail Date','Downtime (Min)','date_numeric']).values\n",
        "Y_train=train['Downtime (GROUPS)']\n",
        "\n",
        "X_test=test.drop(columns=['Downtime (GROUPS)','Fail Date','Downtime (Min)','date_numeric']).values\n",
        "Y_test=test['Downtime (GROUPS)']\n",
        "\n",
        "# Standardize train and test data\n",
        "\n",
        "standard_X_train_fit=standard.fit(X_train)\n",
        "X_trainT=standard_X_train_fit.transform(X_train)\n",
        "\n",
        "standard_X_test_fit=standard.fit(X_test)\n",
        "X_testT=standard_X_test_fit.transform(X_test)\n",
        "\n",
        "models=[]\n",
        "models.append(('LR', LogisticRegression()))\n",
        "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('CART', DecisionTreeClassifier()))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVM', SVC()))\n",
        "\n",
        "results=[]\n",
        "names=[]\n",
        "for name, model in models:\n",
        "  kfold = KFold(n_splits=12, random_state=7,shuffle=True)\n",
        "  cv_results = cross_val_score(model, X_T, Y, cv=kfold, scoring='accuracy')\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  print('{} : {} (std={})'.format(name,round(cv_results.mean(),3),round(cv_results.std(),2)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4khldzIjQQrZ",
        "outputId": "7af02751-4303-40c1-9211-36640f1f37e5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR : 0.243 (std=0.21)\n",
            "LDA : 0.354 (std=0.33)\n",
            "KNN : 0.326 (std=0.28)\n",
            "CART : 0.347 (std=0.21)\n",
            "NB : 0.292 (std=0.21)\n",
            "SVM : 0.243 (std=0.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting Linear Discrimintion model as the best model due to highest crosss validation score"
      ],
      "metadata": {
        "id": "1NyCV-uXS1Yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model=LinearDiscriminantAnalysis()\n",
        "model.fit(X_trainT,Y_train)\n",
        "Y_predict=model.predict(X_testT)\n",
        "\n",
        "target_names =['Low', 'Med', 'High', 'Very High']\n",
        "print(classification_report(Y_test, Y_predict, target_names=target_names))\n",
        "print (confusion_matrix(Y_predict,Y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwJZK2y-PZV9",
        "outputId": "cc0b3edb-3f53-40ea-a76b-a0eee9aa066d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Low       1.00      0.33      0.50         3\n",
            "         Med       0.00      0.00      0.00         2\n",
            "        High       0.00      0.00      0.00         1\n",
            "   Very High       0.00      0.00      0.00         2\n",
            "\n",
            "    accuracy                           0.12         8\n",
            "   macro avg       0.25      0.08      0.12         8\n",
            "weighted avg       0.38      0.12      0.19         8\n",
            "\n",
            "[[1 0 0 0]\n",
            " [2 0 0 1]\n",
            " [0 2 0 1]\n",
            " [0 0 1 0]]\n"
          ]
        }
      ]
    }
  ]
}